{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aKHIi5lqoIzT"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import torch.autograd as ag\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TletFT_UoIzY"
      },
      "source": [
        "# Deep Learning for NLP - lab exercise 1\n",
        "\n",
        "In this first lab exercise we will implement a simple bag-of-word classifier, i.e. a classifier that ignores the sequential structure of the sentence, and a classifier based on a convolutional neural network (CNN). The goal is to predict if a sentence is a positive or negative review of a movie. We will use a dataset constructed from IMDB.\n",
        "\n",
        "1. Load and clean the data\n",
        "2. Preprocess the data for the NN\n",
        "3. Module definition\n",
        "4. Train the network!   \n",
        "\n",
        "We will implement this model with Pytorch, the most popular deep learning framework for Natural Language Processing. You can use the following links for help:\n",
        "\n",
        "- turorials: http://pytorch.org/tutorials/   \n",
        "- documentation: http://pytorch.org/docs/master/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FNcQ-xHoIzd"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LiN4fXToIze"
      },
      "source": [
        "The data can be download here: http://caio-corro.fr/dl4nlp/imdb.zip   \n",
        "\n",
        "There are two files: one with positive reviews (imdb.pos) and one with negative reviews (imdb.neg). Each file contains 300000 reviews, one per line.   \n",
        "\n",
        "The following functions can be used to load and clean the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7Pu5WMnCoIzf"
      },
      "outputs": [],
      "source": [
        "# Tokenize a sentence\n",
        "def clean_str(string, tolower=True):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning.\n",
        "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    if tolower:\n",
        "        string = string.lower()\n",
        "    return string.strip()\n",
        "\n",
        "\n",
        "# reads the content of the file passed as an argument.\n",
        "# if limit > 0, this function will return only the first \"limit\" sentences in the file.\n",
        "def loadTexts(filename, limit=-1):\n",
        "    dataset=[]\n",
        "    with open(filename) as f:\n",
        "        line = f.readline()\n",
        "        cpt=1\n",
        "        skip=0\n",
        "        while line :\n",
        "            cleanline = clean_str(f.readline()).split()\n",
        "            if cleanline:\n",
        "                dataset.append(cleanline)\n",
        "            else:\n",
        "                line = f.readline()\n",
        "                skip+=1\n",
        "                continue\n",
        "            if limit > 0 and cpt >= limit:\n",
        "                break\n",
        "            line = f.readline()\n",
        "            cpt+=1\n",
        "\n",
        "        print(\"Load \", cpt, \" lines from \", filename , \" / \", skip ,\" lines discarded\")\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDPngpkEoIzh"
      },
      "source": [
        "The following cell load the first 5000 sentences in each review set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekHI7M55oIzi",
        "outputId": "e79a6923-d0cb-4ef3-edf8-2c1dd29ddf51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load  5000  lines from  ./imdb.pos  /  1  lines discarded\n",
            "Load  5000  lines from  ./imdb.neg  /  1  lines discarded\n"
          ]
        }
      ],
      "source": [
        "LIM = 5000\n",
        "txtfile = './imdb.pos'  # path of the file containing positive reviews\n",
        "postxt = loadTexts(txtfile,limit=LIM)\n",
        "\n",
        "txtfile = './imdb.neg' # path of the file containing negative reviews\n",
        "negtxt = loadTexts(txtfile,limit=LIM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kahHrtc2oIzj"
      },
      "source": [
        "Split the data between train / dev / test, for example by creating lists txt_train, label_train, txt_dev, ... You should take care to keep a 50/50 ratio between positive and negative instances in each set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XlJO0rMyoIzk"
      },
      "outputs": [],
      "source": [
        "# 70% des data pour train, 15% pour dev et test\n",
        "txt_train = postxt[:int(LIM*0.7)] + negtxt[:int(LIM*0.7)]\n",
        "txt_dev = postxt[int(LIM*0.7):int(LIM*0.85)] + negtxt[int(LIM*0.7):int(LIM*0.85)]\n",
        "txt_test = postxt[int(LIM*0.85):] + negtxt[int(LIM*0.85):]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def replace_with_unk(sentence, unk_prob=0.025):\n",
        "    return [word if random.random() > unk_prob else '<unk>' for word in sentence]\n",
        "#fonction pour placer de maniere aleatoire des \"unk\"\n",
        "train_data_with_unk = [replace_with_unk(sentence) for sentence in txt_train]\n",
        "'''"
      ],
      "metadata": {
        "id": "djH8e5CqJ1lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# on fait les labels\n",
        "def create_labels(texts, label):\n",
        "    return [label] * len(texts)\n",
        "\n",
        "labels_train = create_labels(postxt[:int(LIM*0.7)], 1) + create_labels(negtxt[:int(LIM*0.7)], 0)\n",
        "labels_dev = create_labels(postxt[int(LIM*0.7):int(LIM*0.85)], 1) + create_labels(negtxt[int(LIM*0.7):int(LIM*0.85)], 0)\n",
        "labels_test = create_labels(postxt[int(LIM*0.85):], 1) + create_labels(negtxt[int(LIM*0.85):], 0)\n"
      ],
      "metadata": {
        "id": "1dWGIcS2zn3A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK18r0w-oIzm"
      },
      "source": [
        "# Converting data to Pytorch tensors\n",
        "\n",
        "We will first convert data to Pytorch tensors so they can be used in a neural network. To do that, you must first create a dictionnary that will map words to integers. Add to the dictionnary only words that are in the training set (be sure to understand why we do that!).\n",
        "\n",
        "Then, you can convert the data to tensors:\n",
        "\n",
        "- use tensors of longs: both the sentence and the label will be represented as integers, not floats!\n",
        "- these tensors do not require a gradient\n",
        "\n",
        "A tensor representing a sentence is composed of the integer representation of each word, e.g. [10, 256, 3, 4]. Note that some words in the dev and test sets may not be in the dictionnary! (i.e. unknown words) You can just skip them, even if this is a bad idea in general."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kYyIJ5RSoIzn"
      },
      "outputs": [],
      "source": [
        "#on construit un dictionnaire de mot a partir de l'ensemble de train\n",
        "\n",
        "word_dict = {}\n",
        "for sentence in txt_train:\n",
        "    for word in sentence:\n",
        "        if word not in word_dict:\n",
        "            word_dict[word] = len(word_dict) + 1\n",
        "\n",
        "pad_token = '<pad>'\n",
        "word_dict[pad_token] = len(word_dict)\n",
        "unk_token = '<unk>'\n",
        "word_dict[unk_token] = len(word_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convertion data tensor\n",
        "def sentence_to_tensor(sentence, word_dict):\n",
        "    return th.LongTensor([word_dict.get(word, word_dict.get('<unk>', 0)) for word in sentence])\n",
        "#la fonction get permet de mapper les mots de la sentence en fonction du dictionnaire, si le mot n'est pas trouvé dans le dictionnaire on met \"unk\""
      ],
      "metadata": {
        "id": "NVqJQp-TwPNJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch, word_dict, pad_token='<pad>'):\n",
        "\n",
        "    # Récupérer l'indice du token de padding\n",
        "    pad_idx = word_dict.get(pad_token, None)\n",
        "\n",
        "    # Séparer les textes et les étiquettes dans le batch\n",
        "    text_tensors = [sentence_tensor for sentence_tensor, _ in batch]\n",
        "    labels = [label for _, label in batch]\n",
        "\n",
        "    # Ajouter du padding aux textes pour les rendre de taille égale\n",
        "    padded_texts = pad_sequence(text_tensors, batch_first=True, padding_value=pad_idx)\n",
        "\n",
        "    # Convertir les labels en tenseur\n",
        "    labels_tensor = th.tensor(labels, dtype=th.float if isinstance(labels[0], float) else th.long)\n",
        "\n",
        "    return padded_texts, labels_tensor\n"
      ],
      "metadata": {
        "id": "h8eyr-XKK0dX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#on convertit en tensor en ajoutant les labels\n",
        "batch_size = 64\n",
        "train_tensors = [(sentence_to_tensor(sentence, word_dict), label) for sentence, label in zip(txt_train, labels_train)]\n",
        "dev_tensors = [(sentence_to_tensor(sentence, word_dict), label) for sentence, label in zip(txt_dev, labels_dev)]\n",
        "test_tensors = [(sentence_to_tensor(sentence, word_dict), label) for sentence, label in zip(txt_test, labels_test)]\n",
        "train_loader = DataLoader(train_tensors, batch_size=batch_size, shuffle=True, collate_fn=lambda batch: collate_fn(batch, word_dict))\n",
        "dev_loader = DataLoader(dev_tensors, batch_size=batch_size, shuffle=True, collate_fn=lambda batch: collate_fn(batch, word_dict))\n",
        "test_loader = DataLoader(test_tensors, batch_size=batch_size, shuffle=True, collate_fn=lambda batch: collate_fn(batch, word_dict))\n"
      ],
      "metadata": {
        "id": "uS5Fq4kM0TQO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLxxSm1BoIzo"
      },
      "source": [
        "# Neural network definition\n",
        "\n",
        "You need to implement two networks:\n",
        "\n",
        "- a simple bag of word model (note: it may be better to take the mean of input embeddings that the sum)\n",
        "- a simple CNN as described in the course   \n",
        "\n",
        "To simplify code, you can assume the input will always be a single sentence first, and then implement batched inputs. In the case of batched inputs, give to the forward function a (python) list of tensors.\n",
        "\n",
        "The bag of word neural network should be defined as follows:\n",
        "\n",
        "- take as input a tensor that is a sequence of integers indexing word embeddings\n",
        "- retrieve the word embeddings from an embedding table\n",
        "- construct the \"input\" of the MLP by summing (or computing the mean) over all embeddings (i.e. bag-of-word model)\n",
        "- build a hidden represention using a MLP (1 layer? 2 layers? experiment! but maybe first try wihout any hidden layer...)\n",
        "- project the hidden representation to the output space: it is a binary classification task, so the output space is a scalar where a negative (resp. positive) value means the review is negative (resp. positive).\n",
        "\n",
        "\n",
        "The CNN is a little bit more tricky to implement. The goal is that you implement the one presented in the first lecture. Importantly, you should add \"padding\" tokens before and after the sentence so you can have a convolution even when there is a single word in the input. For example, if you input sentence is [\"word\"], you want to instead consider the sentence [\"<BOS>\", \"word\", \"<EOS>\"] if your window is of size 2 or 3. You can do this either directly when you load the data, or you can do that in the neural network module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_ITMA6pgoIzo"
      },
      "outputs": [],
      "source": [
        "# BAG of word classifier\n",
        "vocab_size = len(word_dict)\n",
        "class CBOW_classifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW_classifier, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.fc = nn.Linear(embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()  # pour la classification binaire\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # TODO\n",
        "        # Récupérer les embeddings pour les mots dans la séquence\n",
        "        embedded = self.embeddings(inputs)\n",
        "        #on fait la moyenne des embeddings\n",
        "        sentence_embedding = embedded.mean(dim=1)\n",
        "\n",
        "        hidden = self.fc(sentence_embedding)\n",
        "\n",
        "        output = self.sigmoid(hidden)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BAG of word classifier\n",
        "vocab_size = len(word_dict)\n",
        "class CBOW_classifier_2(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim,hidden_dim):\n",
        "        super(CBOW_classifier_2, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.hidden = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()  # Fonction d'activation pour la couche cachée\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()  # pour la classification binaire\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "       # Obtenir les embeddings pour les mots dans la séquence\n",
        "        embedded = self.embeddings(inputs)\n",
        "        # Calculer la moyenne des embeddings\n",
        "        sentence_embedding = embedded.mean(dim=1)\n",
        "\n",
        "        # Propagation à travers la couche cachée\n",
        "        hidden = self.relu(self.hidden(sentence_embedding))\n",
        "\n",
        "        # Propagation finale\n",
        "        output = self.sigmoid(self.fc(hidden))\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "lLhO7hz_--tU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_one_layer = CBOW_classifier_2(vocab_size=len(word_dict), embedding_dim=300, hidden_dim=150)\n"
      ],
      "metadata": {
        "id": "PLSHwqnb_vCN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modèle CNN pour la classification\n",
        "class CNN_classifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, kernel_size, num_filters):\n",
        "        super(CNN_classifier, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.conv = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=kernel_size)\n",
        "\n",
        "        # Max pooling pour réduire la dimensionnalité\n",
        "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
        "\n",
        "        # Classification finale\n",
        "        self.fc = nn.Linear(num_filters, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embeddings(inputs) # [batch_size, seq_length, embedding_dim]\n",
        "\n",
        "        embedded = embedded.permute(0, 2, 1)  # [batch_size, embedding_dim, seq_length]\n",
        "\n",
        "        # Passer dans la convolution\n",
        "        conv_out = self.conv(embedded)  # [batch_size, num_filters, seq_length - kernel_size + 1]\n",
        "\n",
        "        # Appliquer le pooling\n",
        "        pooled_out = self.pool(conv_out)  # [batch_size, num_filters, 1]\n",
        "\n",
        "        # Passer dans la couche lineaire\n",
        "        pooled_out = pooled_out.view(pooled_out.size(0), -1)  # [batch_size, num_filters]\n",
        "        hidden = self.fc(pooled_out)  # [batch_size, 1]\n",
        "\n",
        "        # Appliquer la fonction d'activation\n",
        "        output = self.sigmoid(hidden)  # [batch_size, 1]\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "g70jv9-sMsIv"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-eInKuMoIzp"
      },
      "source": [
        "# Loss function\n",
        "\n",
        "Create a loss function builder.\n",
        "\n",
        "- Pytorch loss functions are documented here: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
        "- In our case, we are interested in BCELoss and BCEWithLogitsLoss. Read their documentation and choose the one that fits with your network output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cM23UNZZoIzp"
      },
      "outputs": [],
      "source": [
        "def build_loss_function():\n",
        "    return nn.BCELoss()  # BCELoss, applique une perte cross-entropy binaire\n",
        "    # pour BCELoss on doit appliquer la fonction d'activation sigmoid, ce qui n'est pas le cas avec BCEWithLogitsLoss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAKULetmoIzq"
      },
      "source": [
        "# Training loop\n",
        "\n",
        "Write your training loop!\n",
        "\n",
        "- parameterizable number of epochs\n",
        "- at each epoch, print the mean loss and the dev accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lpTJrAtXoIzq"
      },
      "outputs": [],
      "source": [
        "def train(model, train_data, dev_data, loss_fn, optimizer, num_epochs=10, batch_size=64):\n",
        "    \"\"\"\n",
        "\n",
        "    :param model: Le modèle à entraîner (CNN ou bag-of-words).\n",
        "    :param train_data: Données d'entraînement sous forme de liste de tuples (texte, label).\n",
        "    :param dev_data: Données de développement sous forme de liste de tuples (texte, label).\n",
        "    :param loss_fn: Fonction de perte (BCELoss).\n",
        "    :param optimizer: Optimiseur\n",
        "    :param num_epochs: Nombre d'époques pour l'entraînement.\n",
        "    :param batch_size: Taille du batch pour l'entraînement.\n",
        "    \"\"\"\n",
        "\n",
        "    #  DataLoader pour l'entraînement et la validation, on realise ici le shuffle des données.\n",
        "    train_loader = train_data\n",
        "    dev_loader =  dev_data\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        # Entraîner le modèle par lots\n",
        "        for texts, labels in train_loader:\n",
        "            optimizer.zero_grad()  # Réinitialiser les gradients\n",
        "            outputs = model(texts)  # Propagation avant\n",
        "            loss = loss_fn(outputs.squeeze(), labels.float())  # Calcul de la perte\n",
        "            loss.backward()  # backpropagation\n",
        "            optimizer.step()  # Mise à jour des poids\n",
        "\n",
        "            epoch_loss += loss.item()  # Ajouter la perte de ce lot\n",
        "            predicted = (outputs.squeeze() > 0.5).float()  # Prédire 0 ou 1 avec un seuil de 0.5\n",
        "            correct_train += (predicted == labels).sum().item()  # Nombre de prédictions correctes\n",
        "            total_train += labels.size(0)  # Nombre total d'exemples\n",
        "\n",
        "        # Calcul de la précision sur l'ensemble d'entraînement\n",
        "        train_accuracy = correct_train / total_train * 100  #pour afficher en pourcentage\n",
        "\n",
        "        # Évaluation sur l'ensemble de validation (dev)\n",
        "        model.eval()\n",
        "        correct_dev = 0\n",
        "        total_dev = 0\n",
        "\n",
        "        with th.no_grad():  # Désactiver les gradients pour la validation\n",
        "            for texts, labels in dev_loader:\n",
        "                outputs = model(texts)\n",
        "                predicted = (outputs.squeeze() > 0.5).float()\n",
        "                correct_dev += (predicted == labels).sum().item()\n",
        "                total_dev += labels.size(0)\n",
        "\n",
        "        dev_accuracy = correct_dev / total_dev * 100\n",
        "\n",
        "        # Affichage des résultats\n",
        "        print(f\"Époque {epoch+1}/{num_epochs}\")\n",
        "        print(f\"  Perte entraînement: {epoch_loss/len(train_loader):.4f}\")\n",
        "        print(f\"  Précision entraînement: {train_accuracy:.2f}%\")\n",
        "        print(f\"  Précision validation: {dev_accuracy:.2f}%\\n\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_data, loss_fn):\n",
        "\n",
        "    model.eval()\n",
        "    correct_test = 0\n",
        "    total_test = 0\n",
        "    test_loss = 0.0\n",
        "\n",
        "    with th.no_grad():\n",
        "        for texts, labels in test_data:\n",
        "            outputs = model(texts)\n",
        "\n",
        "            loss = loss_fn(outputs.squeeze(), labels.float())\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calcul des prédictions\n",
        "            predicted = (outputs.squeeze() > 0.5).float()\n",
        "\n",
        "            # Compter le nombre de prédictions correctes\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "            total_test += labels.size(0)\n",
        "\n",
        "    test_accuracy = correct_test / total_test * 100\n",
        "    avg_test_loss = test_loss / len(test_data)\n",
        "\n",
        "    print(f\"Perte de test: {avg_test_loss:.4f}\")\n",
        "    print(f\"Précision de test: {test_accuracy:.2f}%\")\n",
        "\n",
        "    return test_accuracy, avg_test_loss\n"
      ],
      "metadata": {
        "id": "99EPvqi64nNj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = build_loss_function()\n",
        "train_data = train_tensors\n",
        "dev_data = dev_tensors\n",
        "model_1 = CBOW_classifier(len(word_dict), 300)\n",
        "#model_2 = CNN_classifier(len(word_dict), 300, 5, 50)\n",
        "optimizer = th.optim.Adam(model_1.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "n0CJehGORR4E"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_train = train(model_1, train_loader, dev_loader, loss_fn, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csr5X4FeQvg_",
        "outputId": "f530b265-f313-4f4c-b576-7160ab7f580f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Époque 1/10\n",
            "  Perte entraînement: 0.6853\n",
            "  Précision entraînement: 55.49%\n",
            "  Précision validation: 66.13%\n",
            "\n",
            "Époque 2/10\n",
            "  Perte entraînement: 0.6575\n",
            "  Précision entraînement: 66.24%\n",
            "  Précision validation: 64.80%\n",
            "\n",
            "Époque 3/10\n",
            "  Perte entraînement: 0.6207\n",
            "  Précision entraînement: 73.43%\n",
            "  Précision validation: 69.33%\n",
            "\n",
            "Époque 4/10\n",
            "  Perte entraînement: 0.5766\n",
            "  Précision entraînement: 76.96%\n",
            "  Précision validation: 73.47%\n",
            "\n",
            "Époque 5/10\n",
            "  Perte entraînement: 0.5280\n",
            "  Précision entraînement: 80.37%\n",
            "  Précision validation: 76.60%\n",
            "\n",
            "Époque 6/10\n",
            "  Perte entraînement: 0.4855\n",
            "  Précision entraînement: 82.01%\n",
            "  Précision validation: 77.33%\n",
            "\n",
            "Époque 7/10\n",
            "  Perte entraînement: 0.4425\n",
            "  Précision entraînement: 85.24%\n",
            "  Précision validation: 79.00%\n",
            "\n",
            "Époque 8/10\n",
            "  Perte entraînement: 0.4057\n",
            "  Précision entraînement: 86.57%\n",
            "  Précision validation: 78.13%\n",
            "\n",
            "Époque 9/10\n",
            "  Perte entraînement: 0.3742\n",
            "  Précision entraînement: 87.97%\n",
            "  Précision validation: 78.87%\n",
            "\n",
            "Époque 10/10\n",
            "  Perte entraînement: 0.3440\n",
            "  Précision entraînement: 89.04%\n",
            "  Précision validation: 78.67%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy, loss = test(model_1_train, test_loader, loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCgqPXRDBQZN",
        "outputId": "ec321991-9bc3-4e3a-9cf2-59f3fd50b882"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perte de test: 0.5678\n",
            "Précision de test: 76.87%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lDJnPOO0BKaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model_2, train_loader, dev_loader, loss_fn, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91tmCa9LLxcQ",
        "outputId": "5bbd8949-50c6-4292-9aa0-457c5488db51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Époque 1/10\n",
            "  Perte entraînement: 0.7073\n",
            "  Précision entraînement: 49.40%\n",
            "  Précision validation: 48.80%\n",
            "\n",
            "Époque 2/10\n",
            "  Perte entraînement: 0.7074\n",
            "  Précision entraînement: 49.43%\n",
            "  Précision validation: 48.80%\n",
            "\n",
            "Époque 3/10\n",
            "  Perte entraînement: 0.7072\n",
            "  Précision entraînement: 49.47%\n",
            "  Précision validation: 48.60%\n",
            "\n",
            "Époque 4/10\n",
            "  Perte entraînement: 0.7073\n",
            "  Précision entraînement: 49.39%\n",
            "  Précision validation: 48.93%\n",
            "\n",
            "Époque 5/10\n",
            "  Perte entraînement: 0.7073\n",
            "  Précision entraînement: 49.46%\n",
            "  Précision validation: 48.73%\n",
            "\n",
            "Époque 6/10\n",
            "  Perte entraînement: 0.7073\n",
            "  Précision entraînement: 49.43%\n",
            "  Précision validation: 48.87%\n",
            "\n",
            "Époque 7/10\n",
            "  Perte entraînement: 0.7072\n",
            "  Précision entraînement: 49.41%\n",
            "  Précision validation: 48.80%\n",
            "\n",
            "Époque 8/10\n",
            "  Perte entraînement: 0.7074\n",
            "  Précision entraînement: 49.33%\n",
            "  Précision validation: 48.73%\n",
            "\n",
            "Époque 9/10\n",
            "  Perte entraînement: 0.7073\n",
            "  Précision entraînement: 49.37%\n",
            "  Précision validation: 48.80%\n",
            "\n",
            "Époque 10/10\n",
            "  Perte entraînement: 0.7073\n",
            "  Précision entraînement: 49.40%\n",
            "  Précision validation: 48.80%\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}